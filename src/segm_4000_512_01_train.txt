[02/28 15:21:00 d2.engine.defaults]: Model:
GeneralizedRCNN(
  (backbone): FPN(
    (fpn_lateral2): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1))
    (fpn_output2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (fpn_lateral3): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1))
    (fpn_output3): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (fpn_lateral4): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1))
    (fpn_output4): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (fpn_lateral5): Conv2d(2048, 256, kernel_size=(1, 1), stride=(1, 1))
    (fpn_output5): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (top_block): LastLevelMaxPool()
    (bottom_up): ResNet(
      (stem): BasicStem(
        (conv1): Conv2d(
          3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False
          (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)
        )
      )
      (res2): Sequential(
        (0): BottleneckBlock(
          (shortcut): Conv2d(
            64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv1): Conv2d(
            64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)
          )
          (conv2): Conv2d(
            64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)
          )
          (conv3): Conv2d(
            64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
        )
        (1): BottleneckBlock(
          (conv1): Conv2d(
            256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)
          )
          (conv2): Conv2d(
            64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)
          )
          (conv3): Conv2d(
            64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
        )
        (2): BottleneckBlock(
          (conv1): Conv2d(
            256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)
          )
          (conv2): Conv2d(
            64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)
          )
          (conv3): Conv2d(
            64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
        )
      )
      (res3): Sequential(
        (0): BottleneckBlock(
          (shortcut): Conv2d(
            256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False
            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
          )
          (conv1): Conv2d(
            256, 128, kernel_size=(1, 1), stride=(2, 2), bias=False
            (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)
          )
          (conv2): Conv2d(
            128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)
          )
          (conv3): Conv2d(
            128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
          )
        )
        (1): BottleneckBlock(
          (conv1): Conv2d(
            512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)
          )
          (conv2): Conv2d(
            128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)
          )
          (conv3): Conv2d(
            128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
          )
        )
        (2): BottleneckBlock(
          (conv1): Conv2d(
            512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)
          )
          (conv2): Conv2d(
            128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)
          )
          (conv3): Conv2d(
            128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
          )
        )
        (3): BottleneckBlock(
          (conv1): Conv2d(
            512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)
          )
          (conv2): Conv2d(
            128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)
          )
          (conv3): Conv2d(
            128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
          )
        )
      )
      (res4): Sequential(
        (0): BottleneckBlock(
          (shortcut): Conv2d(
            512, 1024, kernel_size=(1, 1), stride=(2, 2), bias=False
            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
          )
          (conv1): Conv2d(
            512, 256, kernel_size=(1, 1), stride=(2, 2), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv2): Conv2d(
            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv3): Conv2d(
            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
          )
        )
        (1): BottleneckBlock(
          (conv1): Conv2d(
            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv2): Conv2d(
            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv3): Conv2d(
            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
          )
        )
        (2): BottleneckBlock(
          (conv1): Conv2d(
            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv2): Conv2d(
            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv3): Conv2d(
            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
          )
        )
        (3): BottleneckBlock(
          (conv1): Conv2d(
            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv2): Conv2d(
            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv3): Conv2d(
            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
          )
        )
        (4): BottleneckBlock(
          (conv1): Conv2d(
            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv2): Conv2d(
            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv3): Conv2d(
            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
          )
        )
        (5): BottleneckBlock(
          (conv1): Conv2d(
            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv2): Conv2d(
            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv3): Conv2d(
            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
          )
        )
      )
      (res5): Sequential(
        (0): BottleneckBlock(
          (shortcut): Conv2d(
            1024, 2048, kernel_size=(1, 1), stride=(2, 2), bias=False
            (norm): FrozenBatchNorm2d(num_features=2048, eps=1e-05)
          )
          (conv1): Conv2d(
            1024, 512, kernel_size=(1, 1), stride=(2, 2), bias=False
            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
          )
          (conv2): Conv2d(
            512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
          )
          (conv3): Conv2d(
            512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=2048, eps=1e-05)
          )
        )
        (1): BottleneckBlock(
          (conv1): Conv2d(
            2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
          )
          (conv2): Conv2d(
            512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
          )
          (conv3): Conv2d(
            512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=2048, eps=1e-05)
          )
        )
        (2): BottleneckBlock(
          (conv1): Conv2d(
            2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
          )
          (conv2): Conv2d(
            512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
          )
          (conv3): Conv2d(
            512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=2048, eps=1e-05)
          )
        )
      )
    )
  )
  (proposal_generator): RPN(
    (rpn_head): StandardRPNHead(
      (conv): Conv2d(
        256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)
        (activation): ReLU()
      )
      (objectness_logits): Conv2d(256, 3, kernel_size=(1, 1), stride=(1, 1))
      (anchor_deltas): Conv2d(256, 12, kernel_size=(1, 1), stride=(1, 1))
    )
    (anchor_generator): DefaultAnchorGenerator(
      (cell_anchors): BufferList()
    )
  )
  (roi_heads): StandardROIHeads(
    (box_pooler): ROIPooler(
      (level_poolers): ModuleList(
        (0): ROIAlign(output_size=(7, 7), spatial_scale=0.25, sampling_ratio=0, aligned=True)
        (1): ROIAlign(output_size=(7, 7), spatial_scale=0.125, sampling_ratio=0, aligned=True)
        (2): ROIAlign(output_size=(7, 7), spatial_scale=0.0625, sampling_ratio=0, aligned=True)
        (3): ROIAlign(output_size=(7, 7), spatial_scale=0.03125, sampling_ratio=0, aligned=True)
      )
    )
    (box_head): FastRCNNConvFCHead(
      (flatten): Flatten(start_dim=1, end_dim=-1)
      (fc1): Linear(in_features=12544, out_features=1024, bias=True)
      (fc_relu1): ReLU()
      (fc2): Linear(in_features=1024, out_features=1024, bias=True)
      (fc_relu2): ReLU()
    )
    (box_predictor): FastRCNNOutputLayers(
      (cls_score): Linear(in_features=1024, out_features=2, bias=True)
      (bbox_pred): Linear(in_features=1024, out_features=4, bias=True)
    )
    (mask_pooler): ROIPooler(
      (level_poolers): ModuleList(
        (0): ROIAlign(output_size=(14, 14), spatial_scale=0.25, sampling_ratio=0, aligned=True)
        (1): ROIAlign(output_size=(14, 14), spatial_scale=0.125, sampling_ratio=0, aligned=True)
        (2): ROIAlign(output_size=(14, 14), spatial_scale=0.0625, sampling_ratio=0, aligned=True)
        (3): ROIAlign(output_size=(14, 14), spatial_scale=0.03125, sampling_ratio=0, aligned=True)
      )
    )
    (mask_head): MaskRCNNConvUpsampleHead(
      (mask_fcn1): Conv2d(
        256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)
        (activation): ReLU()
      )
      (mask_fcn2): Conv2d(
        256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)
        (activation): ReLU()
      )
      (mask_fcn3): Conv2d(
        256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)
        (activation): ReLU()
      )
      (mask_fcn4): Conv2d(
        256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)
        (activation): ReLU()
      )
      (deconv): ConvTranspose2d(256, 256, kernel_size=(2, 2), stride=(2, 2))
      (deconv_relu): ReLU()
      (predictor): Conv2d(256, 1, kernel_size=(1, 1), stride=(1, 1))
    )
  )
)
[02/28 15:21:26 d2.data.build]: Removed 0 images with no usable annotations. 56 images left.
[02/28 15:21:26 d2.data.build]: Distribution of instances among all 1 categories:
|  category  | #instances   |
|:----------:|:-------------|
| Hieroglyph | 14100        |
|            |              |
[02/28 15:21:26 d2.data.dataset_mapper]: [DatasetMapper] Augmentations used in training: [ResizeShortestEdge(short_edge_length=(640, 672, 704, 736, 768, 800), max_size=1333, sample_style='choice'), RandomFlip()]
[02/28 15:21:26 d2.data.build]: Using training sampler TrainingSampler
[02/28 15:21:26 d2.data.common]: Serializing the dataset using: <class 'detectron2.data.common._TorchSerializedList'>
[02/28 15:21:26 d2.data.common]: Serializing 56 elements to byte tensors and concatenating them all ...
[02/28 15:21:26 d2.data.common]: Serialized dataset takes 2.38 MiB
[02/28 15:21:26 d2.data.build]: Making batched data loader with batch_size=2
[02/28 15:21:26 d2.checkpoint.detection_checkpoint]: [DetectionCheckpointer] Loading from https://dl.fbaipublicfiles.com/detectron2/COCO-InstanceSegmentation/mask_rcnn_R_50_FPN_3x/137849600/model_final_f10217.pkl ...

model_final_f10217.pkl: 178MB [00:00, 217MB/s]                           
WARNING:fvcore.common.checkpoint:Skip loading parameter 'roi_heads.box_predictor.cls_score.weight' to the model due to incompatible shapes: (81, 1024) in the checkpoint but (2, 1024) in the model! You might want to double check if this is expected.
WARNING:fvcore.common.checkpoint:Skip loading parameter 'roi_heads.box_predictor.cls_score.bias' to the model due to incompatible shapes: (81,) in the checkpoint but (2,) in the model! You might want to double check if this is expected.
WARNING:fvcore.common.checkpoint:Skip loading parameter 'roi_heads.box_predictor.bbox_pred.weight' to the model due to incompatible shapes: (320, 1024) in the checkpoint but (4, 1024) in the model! You might want to double check if this is expected.
WARNING:fvcore.common.checkpoint:Skip loading parameter 'roi_heads.box_predictor.bbox_pred.bias' to the model due to incompatible shapes: (320,) in the checkpoint but (4,) in the model! You might want to double check if this is expected.
WARNING:fvcore.common.checkpoint:Skip loading parameter 'roi_heads.mask_head.predictor.weight' to the model due to incompatible shapes: (80, 256, 1, 1) in the checkpoint but (1, 256, 1, 1) in the model! You might want to double check if this is expected.
WARNING:fvcore.common.checkpoint:Skip loading parameter 'roi_heads.mask_head.predictor.bias' to the model due to incompatible shapes: (80,) in the checkpoint but (1,) in the model! You might want to double check if this is expected.
WARNING:fvcore.common.checkpoint:Some model parameters or buffers are not found in the checkpoint:
roi_heads.box_predictor.bbox_pred.{bias, weight}
roi_heads.box_predictor.cls_score.{bias, weight}
roi_heads.mask_head.predictor.{bias, weight}

[02/28 15:21:27 d2.engine.train_loop]: Starting training from iteration 0

/usr/local/lib/python3.11/dist-packages/torch/functional.py:534: UserWarning: torch.meshgrid: in an upcoming release, it will be required to pass the indexing argument. (Triggered internally at ../aten/src/ATen/native/TensorShape.cpp:3595.)
  return _VF.meshgrid(tensors, **kwargs)  # type: ignore[attr-defined]

[02/28 15:21:45 d2.utils.events]:  eta: 0:33:25  iter: 19  total_loss: 2.491  loss_cls: 0.6393  loss_box_reg: 0.5443  loss_mask: 0.671  loss_rpn_cls: 0.3354  loss_rpn_loc: 0.2635    time: 0.6669  last_time: 0.7353  data_time: 0.0339  last_data_time: 0.0187   lr: 0.00019981  max_mem: 4343M
[02/28 15:22:03 d2.utils.events]:  eta: 0:33:40  iter: 39  total_loss: 1.951  loss_cls: 0.5375  loss_box_reg: 0.5556  loss_mask: 0.4909  loss_rpn_cls: 0.1432  loss_rpn_loc: 0.212    time: 0.6776  last_time: 0.7014  data_time: 0.0100  last_data_time: 0.0015   lr: 0.00039961  max_mem: 4567M
[02/28 15:22:17 d2.utils.events]:  eta: 0:33:52  iter: 59  total_loss: 1.683  loss_cls: 0.478  loss_box_reg: 0.5494  loss_mask: 0.3859  loss_rpn_cls: 0.1051  loss_rpn_loc: 0.1982    time: 0.6914  last_time: 0.6813  data_time: 0.0162  last_data_time: 0.0062   lr: 0.00059941  max_mem: 4567M
[02/28 15:22:31 d2.utils.events]:  eta: 0:33:29  iter: 79  total_loss: 1.555  loss_cls: 0.4247  loss_box_reg: 0.5134  loss_mask: 0.3594  loss_rpn_cls: 0.08063  loss_rpn_loc: 0.1751    time: 0.6927  last_time: 0.8917  data_time: 0.0124  last_data_time: 0.0077   lr: 0.00079921  max_mem: 4567M
[02/28 15:22:45 d2.utils.events]:  eta: 0:33:11  iter: 99  total_loss: 1.414  loss_cls: 0.3367  loss_box_reg: 0.4945  loss_mask: 0.3255  loss_rpn_cls: 0.07436  loss_rpn_loc: 0.1615    time: 0.6886  last_time: 0.6114  data_time: 0.0128  last_data_time: 0.0039   lr: 0.00099901  max_mem: 4567M
[02/28 15:22:59 d2.utils.events]:  eta: 0:33:08  iter: 119  total_loss: 1.303  loss_cls: 0.3148  loss_box_reg: 0.4785  loss_mask: 0.2831  loss_rpn_cls: 0.04764  loss_rpn_loc: 0.1421    time: 0.6941  last_time: 0.7254  data_time: 0.0136  last_data_time: 0.0139   lr: 0.0011988  max_mem: 4567M
[02/28 15:23:13 d2.utils.events]:  eta: 0:33:00  iter: 139  total_loss: 1.178  loss_cls: 0.3026  loss_box_reg: 0.4522  loss_mask: 0.2721  loss_rpn_cls: 0.04454  loss_rpn_loc: 0.1433    time: 0.6957  last_time: 0.7473  data_time: 0.0137  last_data_time: 0.0217   lr: 0.0013986  max_mem: 4567M
[02/28 15:23:28 d2.utils.events]:  eta: 0:32:54  iter: 159  total_loss: 1.022  loss_cls: 0.2385  loss_box_reg: 0.3709  loss_mask: 0.2438  loss_rpn_cls: 0.03771  loss_rpn_loc: 0.143    time: 0.6994  last_time: 0.7260  data_time: 0.0144  last_data_time: 0.0064   lr: 0.0015984  max_mem: 4581M
[02/28 15:23:42 d2.utils.events]:  eta: 0:32:40  iter: 179  total_loss: 1.063  loss_cls: 0.2533  loss_box_reg: 0.3809  loss_mask: 0.2477  loss_rpn_cls: 0.04028  loss_rpn_loc: 0.147    time: 0.7019  last_time: 0.8564  data_time: 0.0140  last_data_time: 0.0102   lr: 0.0017982  max_mem: 4581M
[02/28 15:23:56 d2.utils.events]:  eta: 0:32:25  iter: 199  total_loss: 0.9696  loss_cls: 0.2218  loss_box_reg: 0.3522  loss_mask: 0.2258  loss_rpn_cls: 0.03394  loss_rpn_loc: 0.1436    time: 0.7020  last_time: 0.9811  data_time: 0.0134  last_data_time: 0.0323   lr: 0.001998  max_mem: 4581M
[02/28 15:24:11 d2.utils.events]:  eta: 0:32:12  iter: 219  total_loss: 0.9178  loss_cls: 0.1929  loss_box_reg: 0.3384  loss_mask: 0.2186  loss_rpn_cls: 0.03075  loss_rpn_loc: 0.1371    time: 0.7035  last_time: 0.6456  data_time: 0.0137  last_data_time: 0.0067   lr: 0.0021978  max_mem: 4736M
[02/28 15:24:25 d2.utils.events]:  eta: 0:31:59  iter: 239  total_loss: 0.8758  loss_cls: 0.199  loss_box_reg: 0.3197  loss_mask: 0.2084  loss_rpn_cls: 0.03147  loss_rpn_loc: 0.1403    time: 0.7030  last_time: 0.5695  data_time: 0.0124  last_data_time: 0.0051   lr: 0.0023976  max_mem: 4736M
[02/28 15:24:38 d2.utils.events]:  eta: 0:31:45  iter: 259  total_loss: 0.8819  loss_cls: 0.177  loss_box_reg: 0.3229  loss_mask: 0.2132  loss_rpn_cls: 0.02947  loss_rpn_loc: 0.1349    time: 0.7020  last_time: 0.6853  data_time: 0.0142  last_data_time: 0.0118   lr: 0.0025974  max_mem: 4736M
[02/28 15:24:53 d2.utils.events]:  eta: 0:31:36  iter: 279  total_loss: 0.8271  loss_cls: 0.1742  loss_box_reg: 0.309  loss_mask: 0.2013  loss_rpn_cls: 0.02856  loss_rpn_loc: 0.1416    time: 0.7026  last_time: 0.4891  data_time: 0.0156  last_data_time: 0.0040   lr: 0.0027972  max_mem: 4736M
[02/28 15:25:07 d2.utils.events]:  eta: 0:31:33  iter: 299  total_loss: 0.8577  loss_cls: 0.1833  loss_box_reg: 0.3157  loss_mask: 0.1996  loss_rpn_cls: 0.0253  loss_rpn_loc: 0.14    time: 0.7051  last_time: 0.7010  data_time: 0.0109  last_data_time: 0.0058   lr: 0.002997  max_mem: 4736M
[02/28 15:25:21 d2.utils.events]:  eta: 0:31:19  iter: 319  total_loss: 0.8615  loss_cls: 0.1664  loss_box_reg: 0.315  loss_mask: 0.1935  loss_rpn_cls: 0.0342  loss_rpn_loc: 0.1357    time: 0.7042  last_time: 0.7548  data_time: 0.0132  last_data_time: 0.0059   lr: 0.0031968  max_mem: 4736M
[02/28 15:25:36 d2.utils.events]:  eta: 0:31:05  iter: 339  total_loss: 0.8132  loss_cls: 0.1572  loss_box_reg: 0.3048  loss_mask: 0.1897  loss_rpn_cls: 0.03359  loss_rpn_loc: 0.1363    time: 0.7049  last_time: 0.7008  data_time: 0.0126  last_data_time: 0.0169   lr: 0.0033966  max_mem: 4736M
[02/28 15:25:50 d2.utils.events]:  eta: 0:30:51  iter: 359  total_loss: 0.7764  loss_cls: 0.1334  loss_box_reg: 0.3087  loss_mask: 0.1852  loss_rpn_cls: 0.02538  loss_rpn_loc: 0.1378    time: 0.7046  last_time: 0.6789  data_time: 0.0120  last_data_time: 0.0017   lr: 0.0035964  max_mem: 4736M
[02/28 15:26:04 d2.utils.events]:  eta: 0:30:36  iter: 379  total_loss: 0.7677  loss_cls: 0.1641  loss_box_reg: 0.3001  loss_mask: 0.1806  loss_rpn_cls: 0.02325  loss_rpn_loc: 0.129    time: 0.7045  last_time: 0.6882  data_time: 0.0126  last_data_time: 0.0159   lr: 0.0037962  max_mem: 4736M
[02/28 15:26:18 d2.utils.events]:  eta: 0:30:21  iter: 399  total_loss: 0.7842  loss_cls: 0.1422  loss_box_reg: 0.2885  loss_mask: 0.1831  loss_rpn_cls: 0.02647  loss_rpn_loc: 0.1416    time: 0.7050  last_time: 0.6273  data_time: 0.0133  last_data_time: 0.0277   lr: 0.003996  max_mem: 4736M
[02/28 15:26:32 d2.utils.events]:  eta: 0:30:06  iter: 419  total_loss: 0.7435  loss_cls: 0.1309  loss_box_reg: 0.2834  loss_mask: 0.1781  loss_rpn_cls: 0.01692  loss_rpn_loc: 0.1288    time: 0.7042  last_time: 0.7103  data_time: 0.0132  last_data_time: 0.0020   lr: 0.0041958  max_mem: 4736M
[02/28 15:26:46 d2.utils.events]:  eta: 0:29:50  iter: 439  total_loss: 0.7482  loss_cls: 0.1376  loss_box_reg: 0.287  loss_mask: 0.1785  loss_rpn_cls: 0.02356  loss_rpn_loc: 0.1307    time: 0.7040  last_time: 0.7283  data_time: 0.0118  last_data_time: 0.0059   lr: 0.0043956  max_mem: 4736M
[02/28 15:27:00 d2.utils.events]:  eta: 0:29:33  iter: 459  total_loss: 0.7484  loss_cls: 0.1273  loss_box_reg: 0.294  loss_mask: 0.1796  loss_rpn_cls: 0.0157  loss_rpn_loc: 0.1304    time: 0.7034  last_time: 0.5622  data_time: 0.0114  last_data_time: 0.0036   lr: 0.0045954  max_mem: 4775M
[02/28 15:27:14 d2.utils.events]:  eta: 0:29:26  iter: 479  total_loss: 0.7025  loss_cls: 0.1306  loss_box_reg: 0.2704  loss_mask: 0.1709  loss_rpn_cls: 0.01905  loss_rpn_loc: 0.1283    time: 0.7047  last_time: 0.7820  data_time: 0.0158  last_data_time: 0.0233   lr: 0.0047952  max_mem: 4775M
[02/28 15:27:29 d2.utils.events]:  eta: 0:29:14  iter: 499  total_loss: 0.7172  loss_cls: 0.116  loss_box_reg: 0.2744  loss_mask: 0.1741  loss_rpn_cls: 0.02163  loss_rpn_loc: 0.1266    time: 0.7062  last_time: 0.8381  data_time: 0.0142  last_data_time: 0.0324   lr: 0.004995  max_mem: 4775M
[02/28 15:27:43 d2.utils.events]:  eta: 0:29:00  iter: 519  total_loss: 0.7149  loss_cls: 0.1194  loss_box_reg: 0.2626  loss_mask: 0.1676  loss_rpn_cls: 0.02916  loss_rpn_loc: 0.1443    time: 0.7062  last_time: 0.5872  data_time: 0.0136  last_data_time: 0.0146   lr: 0.0051948  max_mem: 4775M
[02/28 15:27:57 d2.utils.events]:  eta: 0:28:47  iter: 539  total_loss: 0.7269  loss_cls: 0.1295  loss_box_reg: 0.2808  loss_mask: 0.169  loss_rpn_cls: 0.02379  loss_rpn_loc: 0.1332    time: 0.7063  last_time: 0.6962  data_time: 0.0142  last_data_time: 0.0024   lr: 0.0053946  max_mem: 4775M
[02/28 15:28:12 d2.utils.events]:  eta: 0:28:32  iter: 559  total_loss: 0.7166  loss_cls: 0.124  loss_box_reg: 0.2953  loss_mask: 0.1651  loss_rpn_cls: 0.01912  loss_rpn_loc: 0.1289    time: 0.7064  last_time: 0.6597  data_time: 0.0144  last_data_time: 0.0175   lr: 0.0055944  max_mem: 4775M
[02/28 15:28:26 d2.utils.events]:  eta: 0:28:19  iter: 579  total_loss: 0.7208  loss_cls: 0.1235  loss_box_reg: 0.2932  loss_mask: 0.1697  loss_rpn_cls: 0.01525  loss_rpn_loc: 0.1262    time: 0.7070  last_time: 0.6855  data_time: 0.0140  last_data_time: 0.0114   lr: 0.0057942  max_mem: 4775M
[02/28 15:28:40 d2.utils.events]:  eta: 0:28:05  iter: 599  total_loss: 0.7309  loss_cls: 0.1197  loss_box_reg: 0.2906  loss_mask: 0.1682  loss_rpn_cls: 0.0231  loss_rpn_loc: 0.1365    time: 0.7066  last_time: 0.7656  data_time: 0.0129  last_data_time: 0.0266   lr: 0.005994  max_mem: 4775M
[02/28 15:28:55 d2.utils.events]:  eta: 0:27:52  iter: 619  total_loss: 0.6898  loss_cls: 0.1123  loss_box_reg: 0.2682  loss_mask: 0.1672  loss_rpn_cls: 0.01694  loss_rpn_loc: 0.1262    time: 0.7075  last_time: 0.6721  data_time: 0.0087  last_data_time: 0.0178   lr: 0.0061938  max_mem: 4775M
[02/28 15:29:09 d2.utils.events]:  eta: 0:27:36  iter: 639  total_loss: 0.6732  loss_cls: 0.0991  loss_box_reg: 0.2553  loss_mask: 0.1679  loss_rpn_cls: 0.01526  loss_rpn_loc: 0.1286    time: 0.7073  last_time: 0.6820  data_time: 0.0128  last_data_time: 0.0058   lr: 0.0063936  max_mem: 4775M
[02/28 15:29:23 d2.utils.events]:  eta: 0:27:22  iter: 659  total_loss: 0.7641  loss_cls: 0.1181  loss_box_reg: 0.2968  loss_mask: 0.1723  loss_rpn_cls: 0.02207  loss_rpn_loc: 0.1316    time: 0.7075  last_time: 0.6101  data_time: 0.0129  last_data_time: 0.0065   lr: 0.0065934  max_mem: 4775M
[02/28 15:29:37 d2.utils.events]:  eta: 0:27:08  iter: 679  total_loss: 0.7377  loss_cls: 0.1041  loss_box_reg: 0.2926  loss_mask: 0.1733  loss_rpn_cls: 0.02091  loss_rpn_loc: 0.1318    time: 0.7066  last_time: 0.5657  data_time: 0.0102  last_data_time: 0.0047   lr: 0.0067932  max_mem: 4775M
[02/28 15:29:51 d2.utils.events]:  eta: 0:26:54  iter: 699  total_loss: 0.6933  loss_cls: 0.0908  loss_box_reg: 0.2853  loss_mask: 0.1606  loss_rpn_cls: 0.0201  loss_rpn_loc: 0.1263    time: 0.7071  last_time: 0.7106  data_time: 0.0140  last_data_time: 0.0043   lr: 0.006993  max_mem: 4775M
[02/28 15:30:05 d2.utils.events]:  eta: 0:26:41  iter: 719  total_loss: 0.7218  loss_cls: 0.1039  loss_box_reg: 0.3009  loss_mask: 0.17  loss_rpn_cls: 0.02653  loss_rpn_loc: 0.1398    time: 0.7074  last_time: 0.6637  data_time: 0.0152  last_data_time: 0.0056   lr: 0.0071928  max_mem: 4775M
[02/28 15:30:19 d2.utils.events]:  eta: 0:26:27  iter: 739  total_loss: 0.9329  loss_cls: 0.209  loss_box_reg: 0.3601  loss_mask: 0.182  loss_rpn_cls: 0.03454  loss_rpn_loc: 0.1477    time: 0.7070  last_time: 0.6773  data_time: 0.0115  last_data_time: 0.0063   lr: 0.0073926  max_mem: 4775M
[02/28 15:30:34 d2.utils.events]:  eta: 0:26:14  iter: 759  total_loss: 0.7914  loss_cls: 0.1398  loss_box_reg: 0.3043  loss_mask: 0.1724  loss_rpn_cls: 0.02474  loss_rpn_loc: 0.1396    time: 0.7070  last_time: 0.7092  data_time: 0.0121  last_data_time: 0.0106   lr: 0.0075924  max_mem: 4775M
[02/28 15:30:48 d2.utils.events]:  eta: 0:26:07  iter: 779  total_loss: 0.6911  loss_cls: 0.1123  loss_box_reg: 0.268  loss_mask: 0.1629  loss_rpn_cls: 0.0239  loss_rpn_loc: 0.1314    time: 0.7078  last_time: 0.9459  data_time: 0.0152  last_data_time: 0.0096   lr: 0.0077922  max_mem: 4775M
[02/28 15:31:02 d2.utils.events]:  eta: 0:25:53  iter: 799  total_loss: 0.6909  loss_cls: 0.08901  loss_box_reg: 0.2805  loss_mask: 0.1633  loss_rpn_cls: 0.01982  loss_rpn_loc: 0.1333    time: 0.7075  last_time: 0.6062  data_time: 0.0143  last_data_time: 0.0072   lr: 0.007992  max_mem: 4775M
[02/28 15:31:16 d2.utils.events]:  eta: 0:25:38  iter: 819  total_loss: 0.6777  loss_cls: 0.1177  loss_box_reg: 0.2867  loss_mask: 0.1589  loss_rpn_cls: 0.01793  loss_rpn_loc: 0.1321    time: 0.7073  last_time: 0.6279  data_time: 0.0119  last_data_time: 0.0384   lr: 0.0081918  max_mem: 4775M
[02/28 15:31:30 d2.utils.events]:  eta: 0:25:22  iter: 839  total_loss: 0.7103  loss_cls: 0.09218  loss_box_reg: 0.2974  loss_mask: 0.1592  loss_rpn_cls: 0.02183  loss_rpn_loc: 0.1291    time: 0.7069  last_time: 0.7336  data_time: 0.0131  last_data_time: 0.0347   lr: 0.0083916  max_mem: 4775M
[02/28 15:31:44 d2.utils.events]:  eta: 0:25:05  iter: 859  total_loss: 0.6434  loss_cls: 0.08766  loss_box_reg: 0.2567  loss_mask: 0.1587  loss_rpn_cls: 0.01843  loss_rpn_loc: 0.1234    time: 0.7066  last_time: 0.6929  data_time: 0.0135  last_data_time: 0.0051   lr: 0.0085914  max_mem: 4775M
[02/28 15:31:58 d2.utils.events]:  eta: 0:24:51  iter: 879  total_loss: 0.6325  loss_cls: 0.07943  loss_box_reg: 0.2528  loss_mask: 0.1502  loss_rpn_cls: 0.01437  loss_rpn_loc: 0.1274    time: 0.7064  last_time: 0.6587  data_time: 0.0141  last_data_time: 0.0090   lr: 0.0087912  max_mem: 4775M
[02/28 15:32:12 d2.utils.events]:  eta: 0:24:37  iter: 899  total_loss: 0.6326  loss_cls: 0.08442  loss_box_reg: 0.2512  loss_mask: 0.1525  loss_rpn_cls: 0.01362  loss_rpn_loc: 0.1245    time: 0.7064  last_time: 0.6237  data_time: 0.0130  last_data_time: 0.0075   lr: 0.008991  max_mem: 4775M
[02/28 15:32:26 d2.utils.events]:  eta: 0:24:21  iter: 919  total_loss: 0.6903  loss_cls: 0.09576  loss_box_reg: 0.291  loss_mask: 0.1484  loss_rpn_cls: 0.0144  loss_rpn_loc: 0.1343    time: 0.7060  last_time: 0.6128  data_time: 0.0131  last_data_time: 0.0067   lr: 0.0091908  max_mem: 4775M
[02/28 15:32:40 d2.utils.events]:  eta: 0:24:09  iter: 939  total_loss: 0.6422  loss_cls: 0.07094  loss_box_reg: 0.2577  loss_mask: 0.1553  loss_rpn_cls: 0.01282  loss_rpn_loc: 0.1257    time: 0.7062  last_time: 0.7222  data_time: 0.0160  last_data_time: 0.0150   lr: 0.0093906  max_mem: 4775M
[02/28 15:32:54 d2.utils.events]:  eta: 0:23:57  iter: 959  total_loss: 0.6066  loss_cls: 0.08212  loss_box_reg: 0.256  loss_mask: 0.1451  loss_rpn_cls: 0.008349  loss_rpn_loc: 0.1245    time: 0.7064  last_time: 0.6069  data_time: 0.0144  last_data_time: 0.0064   lr: 0.0095904  max_mem: 4775M
[02/28 15:33:08 d2.utils.events]:  eta: 0:23:42  iter: 979  total_loss: 0.6755  loss_cls: 0.1083  loss_box_reg: 0.279  loss_mask: 0.1548  loss_rpn_cls: 0.01398  loss_rpn_loc: 0.119    time: 0.7062  last_time: 0.7798  data_time: 0.0135  last_data_time: 0.0115   lr: 0.0097902  max_mem: 4775M
[02/28 15:33:23 d2.utils.events]:  eta: 0:23:28  iter: 999  total_loss: 0.6518  loss_cls: 0.0914  loss_box_reg: 0.264  loss_mask: 0.1587  loss_rpn_cls: 0.01507  loss_rpn_loc: 0.1278    time: 0.7062  last_time: 0.6723  data_time: 0.0124  last_data_time: 0.0035   lr: 0.00999  max_mem: 4775M
[02/28 15:33:37 d2.utils.events]:  eta: 0:23:17  iter: 1019  total_loss: 0.6721  loss_cls: 0.1009  loss_box_reg: 0.2767  loss_mask: 0.1439  loss_rpn_cls: 0.01821  loss_rpn_loc: 0.1437    time: 0.7065  last_time: 0.6555  data_time: 0.0145  last_data_time: 0.0041   lr: 0.01  max_mem: 4775M
[02/28 15:33:51 d2.utils.events]:  eta: 0:23:04  iter: 1039  total_loss: 0.6183  loss_cls: 0.07998  loss_box_reg: 0.2641  loss_mask: 0.1449  loss_rpn_cls: 0.014  loss_rpn_loc: 0.1269    time: 0.7060  last_time: 0.7561  data_time: 0.0114  last_data_time: 0.0079   lr: 0.01  max_mem: 4775M
[02/28 15:34:05 d2.utils.events]:  eta: 0:22:50  iter: 1059  total_loss: 0.6094  loss_cls: 0.07513  loss_box_reg: 0.2382  loss_mask: 0.143  loss_rpn_cls: 0.01318  loss_rpn_loc: 0.1249    time: 0.7063  last_time: 0.7970  data_time: 0.0139  last_data_time: 0.0229   lr: 0.01  max_mem: 4775M
[02/28 15:34:19 d2.utils.events]:  eta: 0:22:37  iter: 1079  total_loss: 0.725  loss_cls: 0.1179  loss_box_reg: 0.3014  loss_mask: 0.1528  loss_rpn_cls: 0.01717  loss_rpn_loc: 0.1379    time: 0.7063  last_time: 0.6460  data_time: 0.0117  last_data_time: 0.0177   lr: 0.01  max_mem: 4775M
[02/28 15:34:33 d2.utils.events]:  eta: 0:22:23  iter: 1099  total_loss: 0.6409  loss_cls: 0.09908  loss_box_reg: 0.2645  loss_mask: 0.1422  loss_rpn_cls: 0.01361  loss_rpn_loc: 0.1269    time: 0.7060  last_time: 0.7506  data_time: 0.0132  last_data_time: 0.0041   lr: 0.01  max_mem: 4775M
[02/28 15:34:47 d2.utils.events]:  eta: 0:22:08  iter: 1119  total_loss: 0.6107  loss_cls: 0.07674  loss_box_reg: 0.246  loss_mask: 0.1453  loss_rpn_cls: 0.01506  loss_rpn_loc: 0.1252    time: 0.7058  last_time: 0.6577  data_time: 0.0131  last_data_time: 0.0018   lr: 0.01  max_mem: 4775M
[02/28 15:35:01 d2.utils.events]:  eta: 0:21:55  iter: 1139  total_loss: 0.6105  loss_cls: 0.08054  loss_box_reg: 0.2514  loss_mask: 0.1479  loss_rpn_cls: 0.01244  loss_rpn_loc: 0.1217    time: 0.7059  last_time: 0.8610  data_time: 0.0167  last_data_time: 0.0310   lr: 0.01  max_mem: 4775M
[02/28 15:35:16 d2.utils.events]:  eta: 0:21:41  iter: 1159  total_loss: 0.5752  loss_cls: 0.07028  loss_box_reg: 0.2401  loss_mask: 0.1426  loss_rpn_cls: 0.0123  loss_rpn_loc: 0.1271    time: 0.7061  last_time: 0.6860  data_time: 0.0135  last_data_time: 0.0127   lr: 0.01  max_mem: 4775M
[02/28 15:35:31 d2.utils.events]:  eta: 0:21:27  iter: 1179  total_loss: 0.5564  loss_cls: 0.0626  loss_box_reg: 0.223  loss_mask: 0.1435  loss_rpn_cls: 0.009765  loss_rpn_loc: 0.1166    time: 0.7070  last_time: 0.7786  data_time: 0.0111  last_data_time: 0.0125   lr: 0.01  max_mem: 4775M
[02/28 15:35:45 d2.utils.events]:  eta: 0:21:15  iter: 1199  total_loss: 0.5617  loss_cls: 0.06942  loss_box_reg: 0.2288  loss_mask: 0.14  loss_rpn_cls: 0.0121  loss_rpn_loc: 0.117    time: 0.7072  last_time: 0.7880  data_time: 0.0135  last_data_time: 0.0111   lr: 0.01  max_mem: 4775M
[02/28 15:36:00 d2.utils.events]:  eta: 0:21:03  iter: 1219  total_loss: 0.5727  loss_cls: 0.07124  loss_box_reg: 0.2293  loss_mask: 0.1505  loss_rpn_cls: 0.01248  loss_rpn_loc: 0.1091    time: 0.7075  last_time: 0.6899  data_time: 0.0134  last_data_time: 0.0200   lr: 0.01  max_mem: 4775M
[02/28 15:36:14 d2.utils.events]:  eta: 0:20:49  iter: 1239  total_loss: 0.6235  loss_cls: 0.07725  loss_box_reg: 0.2556  loss_mask: 0.1499  loss_rpn_cls: 0.01245  loss_rpn_loc: 0.1272    time: 0.7075  last_time: 0.6427  data_time: 0.0142  last_data_time: 0.0200   lr: 0.01  max_mem: 4775M
[02/28 15:36:28 d2.utils.events]:  eta: 0:20:35  iter: 1259  total_loss: 0.5634  loss_cls: 0.05676  loss_box_reg: 0.2428  loss_mask: 0.147  loss_rpn_cls: 0.01445  loss_rpn_loc: 0.1169    time: 0.7075  last_time: 0.8191  data_time: 0.0130  last_data_time: 0.0064   lr: 0.01  max_mem: 4775M
[02/28 15:36:42 d2.utils.events]:  eta: 0:20:21  iter: 1279  total_loss: 0.5314  loss_cls: 0.04807  loss_box_reg: 0.2153  loss_mask: 0.1374  loss_rpn_cls: 0.009573  loss_rpn_loc: 0.1177    time: 0.7076  last_time: 0.6180  data_time: 0.0120  last_data_time: 0.0113   lr: 0.01  max_mem: 4775M
[02/28 15:36:57 d2.utils.events]:  eta: 0:20:05  iter: 1299  total_loss: 0.5376  loss_cls: 0.05715  loss_box_reg: 0.2183  loss_mask: 0.1368  loss_rpn_cls: 0.01145  loss_rpn_loc: 0.1149    time: 0.7078  last_time: 0.6833  data_time: 0.0125  last_data_time: 0.0155   lr: 0.01  max_mem: 4775M
[02/28 15:37:11 d2.utils.events]:  eta: 0:19:51  iter: 1319  total_loss: 0.5278  loss_cls: 0.06882  loss_box_reg: 0.2164  loss_mask: 0.134  loss_rpn_cls: 0.01092  loss_rpn_loc: 0.1112    time: 0.7080  last_time: 0.7726  data_time: 0.0132  last_data_time: 0.0048   lr: 0.01  max_mem: 4775M
[02/28 15:37:25 d2.utils.events]:  eta: 0:19:35  iter: 1339  total_loss: 0.5255  loss_cls: 0.05631  loss_box_reg: 0.2189  loss_mask: 0.1393  loss_rpn_cls: 0.008351  loss_rpn_loc: 0.1068    time: 0.7078  last_time: 0.6341  data_time: 0.0125  last_data_time: 0.0082   lr: 0.01  max_mem: 4775M
[02/28 15:37:40 d2.utils.events]:  eta: 0:19:22  iter: 1359  total_loss: 0.5312  loss_cls: 0.05102  loss_box_reg: 0.2164  loss_mask: 0.1366  loss_rpn_cls: 0.01094  loss_rpn_loc: 0.1163    time: 0.7081  last_time: 0.6149  data_time: 0.0170  last_data_time: 0.0120   lr: 0.01  max_mem: 4775M
[02/28 15:37:54 d2.utils.events]:  eta: 0:19:08  iter: 1379  total_loss: 0.53  loss_cls: 0.06238  loss_box_reg: 0.217  loss_mask: 0.1278  loss_rpn_cls: 0.009813  loss_rpn_loc: 0.1144    time: 0.7080  last_time: 0.6169  data_time: 0.0123  last_data_time: 0.0086   lr: 0.01  max_mem: 4775M
[02/28 15:38:08 d2.utils.events]:  eta: 0:18:54  iter: 1399  total_loss: 0.5561  loss_cls: 0.05647  loss_box_reg: 0.2311  loss_mask: 0.1387  loss_rpn_cls: 0.01168  loss_rpn_loc: 0.1165    time: 0.7080  last_time: 0.6307  data_time: 0.0136  last_data_time: 0.0036   lr: 0.01  max_mem: 4775M
[02/28 15:38:22 d2.utils.events]:  eta: 0:18:40  iter: 1419  total_loss: 0.5505  loss_cls: 0.06513  loss_box_reg: 0.2354  loss_mask: 0.1393  loss_rpn_cls: 0.01122  loss_rpn_loc: 0.1143    time: 0.7081  last_time: 0.7901  data_time: 0.0151  last_data_time: 0.0281   lr: 0.01  max_mem: 4775M
[02/28 15:38:36 d2.utils.events]:  eta: 0:18:26  iter: 1439  total_loss: 0.501  loss_cls: 0.05029  loss_box_reg: 0.2059  loss_mask: 0.1367  loss_rpn_cls: 0.009386  loss_rpn_loc: 0.1118    time: 0.7080  last_time: 0.6508  data_time: 0.0147  last_data_time: 0.0308   lr: 0.01  max_mem: 4775M
[02/28 15:38:50 d2.utils.events]:  eta: 0:18:12  iter: 1459  total_loss: 0.5445  loss_cls: 0.05107  loss_box_reg: 0.229  loss_mask: 0.1419  loss_rpn_cls: 0.01018  loss_rpn_loc: 0.1136    time: 0.7077  last_time: 0.5672  data_time: 0.0124  last_data_time: 0.0067   lr: 0.01  max_mem: 4775M
[02/28 15:39:04 d2.utils.events]:  eta: 0:17:58  iter: 1479  total_loss: 0.5113  loss_cls: 0.05675  loss_box_reg: 0.2095  loss_mask: 0.132  loss_rpn_cls: 0.01065  loss_rpn_loc: 0.112    time: 0.7079  last_time: 0.7970  data_time: 0.0129  last_data_time: 0.0216   lr: 0.01  max_mem: 4775M
[02/28 15:39:19 d2.utils.events]:  eta: 0:17:43  iter: 1499  total_loss: 0.5067  loss_cls: 0.04176  loss_box_reg: 0.2168  loss_mask: 0.1348  loss_rpn_cls: 0.01072  loss_rpn_loc: 0.1155    time: 0.7080  last_time: 0.6618  data_time: 0.0135  last_data_time: 0.0076   lr: 0.01  max_mem: 4775M
[02/28 15:39:33 d2.utils.events]:  eta: 0:17:29  iter: 1519  total_loss: 0.5012  loss_cls: 0.05411  loss_box_reg: 0.2069  loss_mask: 0.1334  loss_rpn_cls: 0.01129  loss_rpn_loc: 0.1081    time: 0.7079  last_time: 0.6739  data_time: 0.0140  last_data_time: 0.0097   lr: 0.01  max_mem: 4775M
[02/28 15:39:46 d2.utils.events]:  eta: 0:17:15  iter: 1539  total_loss: 0.4865  loss_cls: 0.04227  loss_box_reg: 0.2026  loss_mask: 0.1256  loss_rpn_cls: 0.00776  loss_rpn_loc: 0.1088    time: 0.7076  last_time: 0.5902  data_time: 0.0110  last_data_time: 0.0048   lr: 0.01  max_mem: 4775M
[02/28 15:40:01 d2.utils.events]:  eta: 0:17:02  iter: 1559  total_loss: 0.462  loss_cls: 0.04169  loss_box_reg: 0.1899  loss_mask: 0.1249  loss_rpn_cls: 0.007632  loss_rpn_loc: 0.1013    time: 0.7078  last_time: 0.6555  data_time: 0.0133  last_data_time: 0.0022   lr: 0.01  max_mem: 4775M
[02/28 15:40:14 d2.utils.events]:  eta: 0:16:46  iter: 1579  total_loss: 0.5317  loss_cls: 0.04891  loss_box_reg: 0.2159  loss_mask: 0.1346  loss_rpn_cls: 0.009488  loss_rpn_loc: 0.1118    time: 0.7074  last_time: 0.6290  data_time: 0.0104  last_data_time: 0.0142   lr: 0.01  max_mem: 4775M
[02/28 15:40:29 d2.utils.events]:  eta: 0:16:33  iter: 1599  total_loss: 0.4697  loss_cls: 0.04242  loss_box_reg: 0.1919  loss_mask: 0.1319  loss_rpn_cls: 0.007563  loss_rpn_loc: 0.1013    time: 0.7078  last_time: 0.7320  data_time: 0.0149  last_data_time: 0.0122   lr: 0.01  max_mem: 4775M
[02/28 15:40:44 d2.utils.events]:  eta: 0:16:19  iter: 1619  total_loss: 0.4807  loss_cls: 0.05701  loss_box_reg: 0.1971  loss_mask: 0.1305  loss_rpn_cls: 0.007985  loss_rpn_loc: 0.1071    time: 0.7081  last_time: 0.5733  data_time: 0.0144  last_data_time: 0.0058   lr: 0.01  max_mem: 4775M
[02/28 15:40:58 d2.utils.events]:  eta: 0:16:06  iter: 1639  total_loss: 0.5136  loss_cls: 0.05014  loss_box_reg: 0.2182  loss_mask: 0.1328  loss_rpn_cls: 0.01095  loss_rpn_loc: 0.1146    time: 0.7084  last_time: 0.6873  data_time: 0.0103  last_data_time: 0.0044   lr: 0.01  max_mem: 4775M
[02/28 15:41:13 d2.utils.events]:  eta: 0:15:51  iter: 1659  total_loss: 0.5151  loss_cls: 0.04779  loss_box_reg: 0.2103  loss_mask: 0.1261  loss_rpn_cls: 0.01002  loss_rpn_loc: 0.112    time: 0.7083  last_time: 0.6009  data_time: 0.0141  last_data_time: 0.0059   lr: 0.01  max_mem: 4775M
[02/28 15:41:27 d2.utils.events]:  eta: 0:15:37  iter: 1679  total_loss: 0.4832  loss_cls: 0.04269  loss_box_reg: 0.1977  loss_mask: 0.1281  loss_rpn_cls: 0.007596  loss_rpn_loc: 0.1093    time: 0.7086  last_time: 0.8026  data_time: 0.0121  last_data_time: 0.0291   lr: 0.01  max_mem: 4775M
[02/28 15:41:42 d2.utils.events]:  eta: 0:15:23  iter: 1699  total_loss: 0.4876  loss_cls: 0.04998  loss_box_reg: 0.2025  loss_mask: 0.1225  loss_rpn_cls: 0.008112  loss_rpn_loc: 0.1057    time: 0.7088  last_time: 0.7806  data_time: 0.0117  last_data_time: 0.0064   lr: 0.01  max_mem: 4775M
[02/28 15:41:56 d2.utils.events]:  eta: 0:15:08  iter: 1719  total_loss: 0.4824  loss_cls: 0.04284  loss_box_reg: 0.1998  loss_mask: 0.1343  loss_rpn_cls: 0.008766  loss_rpn_loc: 0.1075    time: 0.7086  last_time: 0.7574  data_time: 0.0125  last_data_time: 0.0138   lr: 0.01  max_mem: 4775M
[02/28 15:42:11 d2.utils.events]:  eta: 0:14:55  iter: 1739  total_loss: 0.4681  loss_cls: 0.04736  loss_box_reg: 0.1915  loss_mask: 0.1271  loss_rpn_cls: 0.007827  loss_rpn_loc: 0.1052    time: 0.7091  last_time: 0.8152  data_time: 0.0141  last_data_time: 0.0127   lr: 0.01  max_mem: 4775M
[02/28 15:42:24 d2.utils.events]:  eta: 0:14:40  iter: 1759  total_loss: 0.4802  loss_cls: 0.04518  loss_box_reg: 0.1892  loss_mask: 0.1236  loss_rpn_cls: 0.005916  loss_rpn_loc: 0.1005    time: 0.7088  last_time: 0.6154  data_time: 0.0142  last_data_time: 0.0065   lr: 0.01  max_mem: 4775M
[02/28 15:42:38 d2.utils.events]:  eta: 0:14:25  iter: 1779  total_loss: 0.4757  loss_cls: 0.03653  loss_box_reg: 0.192  loss_mask: 0.1313  loss_rpn_cls: 0.01049  loss_rpn_loc: 0.107    time: 0.7087  last_time: 0.6065  data_time: 0.0114  last_data_time: 0.0078   lr: 0.01  max_mem: 4775M
[02/28 15:42:53 d2.utils.events]:  eta: 0:14:12  iter: 1799  total_loss: 0.4715  loss_cls: 0.04001  loss_box_reg: 0.1904  loss_mask: 0.1232  loss_rpn_cls: 0.009553  loss_rpn_loc: 0.1123    time: 0.7091  last_time: 0.6449  data_time: 0.0158  last_data_time: 0.0173   lr: 0.01  max_mem: 4775M
[02/28 15:43:07 d2.utils.events]:  eta: 0:13:58  iter: 1819  total_loss: 0.4624  loss_cls: 0.05124  loss_box_reg: 0.187  loss_mask: 0.1256  loss_rpn_cls: 0.005258  loss_rpn_loc: 0.09657    time: 0.7091  last_time: 0.7046  data_time: 0.0138  last_data_time: 0.0050   lr: 0.01  max_mem: 4775M
[02/28 15:43:21 d2.utils.events]:  eta: 0:13:44  iter: 1839  total_loss: 0.446  loss_cls: 0.04926  loss_box_reg: 0.1843  loss_mask: 0.1189  loss_rpn_cls: 0.006727  loss_rpn_loc: 0.1045    time: 0.7090  last_time: 0.5856  data_time: 0.0148  last_data_time: 0.0045   lr: 0.01  max_mem: 4775M
[02/28 15:43:36 d2.utils.events]:  eta: 0:13:33  iter: 1859  total_loss: 0.4852  loss_cls: 0.04199  loss_box_reg: 0.1963  loss_mask: 0.1268  loss_rpn_cls: 0.006912  loss_rpn_loc: 0.1085    time: 0.7092  last_time: 0.7785  data_time: 0.0120  last_data_time: 0.0057   lr: 0.01  max_mem: 4775M
[02/28 15:43:50 d2.utils.events]:  eta: 0:13:19  iter: 1879  total_loss: 0.4873  loss_cls: 0.04971  loss_box_reg: 0.2085  loss_mask: 0.1256  loss_rpn_cls: 0.006306  loss_rpn_loc: 0.09627    time: 0.7091  last_time: 0.6905  data_time: 0.0126  last_data_time: 0.0054   lr: 0.01  max_mem: 4775M
[02/28 15:44:05 d2.utils.events]:  eta: 0:13:05  iter: 1899  total_loss: 0.4526  loss_cls: 0.03897  loss_box_reg: 0.1876  loss_mask: 0.124  loss_rpn_cls: 0.006647  loss_rpn_loc: 0.1017    time: 0.7094  last_time: 0.7386  data_time: 0.0138  last_data_time: 0.0055   lr: 0.01  max_mem: 4775M
[02/28 15:44:18 d2.utils.events]:  eta: 0:12:52  iter: 1919  total_loss: 0.4653  loss_cls: 0.04468  loss_box_reg: 0.1846  loss_mask: 0.1316  loss_rpn_cls: 0.01087  loss_rpn_loc: 0.1142    time: 0.7091  last_time: 0.7615  data_time: 0.0126  last_data_time: 0.0238   lr: 0.01  max_mem: 4775M
[02/28 15:44:33 d2.utils.events]:  eta: 0:12:37  iter: 1939  total_loss: 0.4661  loss_cls: 0.0442  loss_box_reg: 0.186  loss_mask: 0.1251  loss_rpn_cls: 0.007467  loss_rpn_loc: 0.09961    time: 0.7092  last_time: 0.7380  data_time: 0.0135  last_data_time: 0.0129   lr: 0.01  max_mem: 4775M
[02/28 15:44:47 d2.utils.events]:  eta: 0:12:24  iter: 1959  total_loss: 0.4571  loss_cls: 0.04064  loss_box_reg: 0.1812  loss_mask: 0.123  loss_rpn_cls: 0.007724  loss_rpn_loc: 0.09438    time: 0.7094  last_time: 0.7457  data_time: 0.0136  last_data_time: 0.0040   lr: 0.01  max_mem: 4775M
[02/28 15:45:02 d2.utils.events]:  eta: 0:12:10  iter: 1979  total_loss: 0.4448  loss_cls: 0.0437  loss_box_reg: 0.1839  loss_mask: 0.1239  loss_rpn_cls: 0.008191  loss_rpn_loc: 0.1035    time: 0.7094  last_time: 0.5854  data_time: 0.0144  last_data_time: 0.0081   lr: 0.01  max_mem: 4775M
[02/28 15:45:16 d2.utils.events]:  eta: 0:11:56  iter: 1999  total_loss: 0.4926  loss_cls: 0.04074  loss_box_reg: 0.2049  loss_mask: 0.1274  loss_rpn_cls: 0.007635  loss_rpn_loc: 0.1063    time: 0.7093  last_time: 0.5756  data_time: 0.0113  last_data_time: 0.0071   lr: 0.01  max_mem: 4775M
[02/28 15:45:30 d2.utils.events]:  eta: 0:11:41  iter: 2019  total_loss: 0.4556  loss_cls: 0.04738  loss_box_reg: 0.1841  loss_mask: 0.1211  loss_rpn_cls: 0.007253  loss_rpn_loc: 0.09614    time: 0.7092  last_time: 0.7037  data_time: 0.0150  last_data_time: 0.0015   lr: 0.01  max_mem: 4775M
[02/28 15:45:44 d2.utils.events]:  eta: 0:11:27  iter: 2039  total_loss: 0.4513  loss_cls: 0.0498  loss_box_reg: 0.18  loss_mask: 0.1291  loss_rpn_cls: 0.006308  loss_rpn_loc: 0.1032    time: 0.7094  last_time: 0.7384  data_time: 0.0120  last_data_time: 0.0166   lr: 0.01  max_mem: 4775M
[02/28 15:45:59 d2.utils.events]:  eta: 0:11:13  iter: 2059  total_loss: 0.4379  loss_cls: 0.04525  loss_box_reg: 0.172  loss_mask: 0.1202  loss_rpn_cls: 0.007609  loss_rpn_loc: 0.09427    time: 0.7096  last_time: 0.7438  data_time: 0.0120  last_data_time: 0.0094   lr: 0.01  max_mem: 4775M
[02/28 15:46:13 d2.utils.events]:  eta: 0:10:59  iter: 2079  total_loss: 0.4415  loss_cls: 0.04586  loss_box_reg: 0.1815  loss_mask: 0.1256  loss_rpn_cls: 0.006896  loss_rpn_loc: 0.0908    time: 0.7095  last_time: 0.5892  data_time: 0.0128  last_data_time: 0.0067   lr: 0.01  max_mem: 4775M
[02/28 15:46:27 d2.utils.events]:  eta: 0:10:46  iter: 2099  total_loss: 0.4313  loss_cls: 0.03619  loss_box_reg: 0.179  loss_mask: 0.1221  loss_rpn_cls: 0.005251  loss_rpn_loc: 0.09346    time: 0.7097  last_time: 0.6762  data_time: 0.0135  last_data_time: 0.0039   lr: 0.01  max_mem: 4775M
[02/28 15:46:42 d2.utils.events]:  eta: 0:10:33  iter: 2119  total_loss: 0.4398  loss_cls: 0.0413  loss_box_reg: 0.1843  loss_mask: 0.1178  loss_rpn_cls: 0.005454  loss_rpn_loc: 0.09335    time: 0.7097  last_time: 0.7340  data_time: 0.0148  last_data_time: 0.0069   lr: 0.01  max_mem: 4775M
[02/28 15:46:56 d2.utils.events]:  eta: 0:10:18  iter: 2139  total_loss: 0.4385  loss_cls: 0.04601  loss_box_reg: 0.183  loss_mask: 0.1205  loss_rpn_cls: 0.007426  loss_rpn_loc: 0.09957    time: 0.7097  last_time: 0.7050  data_time: 0.0123  last_data_time: 0.0037   lr: 0.01  max_mem: 4775M
[02/28 15:47:10 d2.utils.events]:  eta: 0:10:02  iter: 2159  total_loss: 0.4784  loss_cls: 0.04499  loss_box_reg: 0.195  loss_mask: 0.1279  loss_rpn_cls: 0.009246  loss_rpn_loc: 0.09763    time: 0.7096  last_time: 0.7623  data_time: 0.0125  last_data_time: 0.0139   lr: 0.01  max_mem: 4775M
[02/28 15:47:24 d2.utils.events]:  eta: 0:09:48  iter: 2179  total_loss: 0.4465  loss_cls: 0.0379  loss_box_reg: 0.1903  loss_mask: 0.1212  loss_rpn_cls: 0.005492  loss_rpn_loc: 0.09541    time: 0.7096  last_time: 0.6722  data_time: 0.0130  last_data_time: 0.0142   lr: 0.01  max_mem: 4775M
[02/28 15:47:38 d2.utils.events]:  eta: 0:09:33  iter: 2199  total_loss: 0.463  loss_cls: 0.03305  loss_box_reg: 0.183  loss_mask: 0.1176  loss_rpn_cls: 0.009014  loss_rpn_loc: 0.1181    time: 0.7096  last_time: 0.6291  data_time: 0.0134  last_data_time: 0.0051   lr: 0.01  max_mem: 4775M
[02/28 15:47:53 d2.utils.events]:  eta: 0:09:19  iter: 2219  total_loss: 0.4538  loss_cls: 0.03611  loss_box_reg: 0.1939  loss_mask: 0.1217  loss_rpn_cls: 0.007985  loss_rpn_loc: 0.1015    time: 0.7098  last_time: 0.8082  data_time: 0.0147  last_data_time: 0.0261   lr: 0.01  max_mem: 4775M
[02/28 15:48:08 d2.utils.events]:  eta: 0:09:05  iter: 2239  total_loss: 0.4219  loss_cls: 0.03694  loss_box_reg: 0.1759  loss_mask: 0.1194  loss_rpn_cls: 0.006852  loss_rpn_loc: 0.08614    time: 0.7101  last_time: 0.6647  data_time: 0.0152  last_data_time: 0.0110   lr: 0.01  max_mem: 4775M
[02/28 15:48:22 d2.utils.events]:  eta: 0:08:51  iter: 2259  total_loss: 0.4372  loss_cls: 0.03739  loss_box_reg: 0.1798  loss_mask: 0.1229  loss_rpn_cls: 0.005278  loss_rpn_loc: 0.09183    time: 0.7102  last_time: 0.8986  data_time: 0.0130  last_data_time: 0.0264   lr: 0.01  max_mem: 4775M
[02/28 15:48:37 d2.utils.events]:  eta: 0:08:36  iter: 2279  total_loss: 0.4278  loss_cls: 0.03743  loss_box_reg: 0.1799  loss_mask: 0.1192  loss_rpn_cls: 0.006132  loss_rpn_loc: 0.0863    time: 0.7103  last_time: 0.6278  data_time: 0.0140  last_data_time: 0.0093   lr: 0.01  max_mem: 4775M
[02/28 15:48:51 d2.utils.events]:  eta: 0:08:22  iter: 2299  total_loss: 0.3987  loss_cls: 0.03279  loss_box_reg: 0.1622  loss_mask: 0.1191  loss_rpn_cls: 0.006228  loss_rpn_loc: 0.09423    time: 0.7104  last_time: 0.6304  data_time: 0.0134  last_data_time: 0.0068   lr: 0.01  max_mem: 4775M
[02/28 15:49:05 d2.utils.events]:  eta: 0:08:08  iter: 2319  total_loss: 0.4244  loss_cls: 0.03485  loss_box_reg: 0.1677  loss_mask: 0.1185  loss_rpn_cls: 0.006982  loss_rpn_loc: 0.09466    time: 0.7104  last_time: 0.5833  data_time: 0.0220  last_data_time: 0.0025   lr: 0.01  max_mem: 4776M
[02/28 15:49:20 d2.utils.events]:  eta: 0:07:54  iter: 2339  total_loss: 0.4397  loss_cls: 0.03718  loss_box_reg: 0.1831  loss_mask: 0.1173  loss_rpn_cls: 0.006672  loss_rpn_loc: 0.09525    time: 0.7105  last_time: 0.7578  data_time: 0.0126  last_data_time: 0.0043   lr: 0.01  max_mem: 4776M
[02/28 15:49:34 d2.utils.events]:  eta: 0:07:40  iter: 2359  total_loss: 0.4242  loss_cls: 0.04132  loss_box_reg: 0.175  loss_mask: 0.1159  loss_rpn_cls: 0.005321  loss_rpn_loc: 0.09033    time: 0.7106  last_time: 0.7172  data_time: 0.0127  last_data_time: 0.0119   lr: 0.01  max_mem: 4776M
[02/28 15:49:49 d2.utils.events]:  eta: 0:07:26  iter: 2379  total_loss: 0.4219  loss_cls: 0.03269  loss_box_reg: 0.1726  loss_mask: 0.1182  loss_rpn_cls: 0.00494  loss_rpn_loc: 0.09171    time: 0.7107  last_time: 0.7672  data_time: 0.0162  last_data_time: 0.0085   lr: 0.01  max_mem: 4776M
[02/28 15:50:03 d2.utils.events]:  eta: 0:07:12  iter: 2399  total_loss: 0.4175  loss_cls: 0.02867  loss_box_reg: 0.1774  loss_mask: 0.1144  loss_rpn_cls: 0.004342  loss_rpn_loc: 0.08633    time: 0.7107  last_time: 0.6627  data_time: 0.0109  last_data_time: 0.0074   lr: 0.01  max_mem: 4776M
[02/28 15:50:17 d2.utils.events]:  eta: 0:06:57  iter: 2419  total_loss: 0.4769  loss_cls: 0.03476  loss_box_reg: 0.1951  loss_mask: 0.1236  loss_rpn_cls: 0.007928  loss_rpn_loc: 0.1045    time: 0.7106  last_time: 0.6717  data_time: 0.0153  last_data_time: 0.0074   lr: 0.01  max_mem: 4776M
[02/28 15:50:31 d2.utils.events]:  eta: 0:06:43  iter: 2439  total_loss: 0.4306  loss_cls: 0.03566  loss_box_reg: 0.1763  loss_mask: 0.1184  loss_rpn_cls: 0.005728  loss_rpn_loc: 0.09398    time: 0.7106  last_time: 0.7721  data_time: 0.0128  last_data_time: 0.0018   lr: 0.01  max_mem: 4776M
[02/28 15:50:45 d2.utils.events]:  eta: 0:06:28  iter: 2459  total_loss: 0.4213  loss_cls: 0.03993  loss_box_reg: 0.1683  loss_mask: 0.1127  loss_rpn_cls: 0.005652  loss_rpn_loc: 0.09607    time: 0.7104  last_time: 0.7285  data_time: 0.0120  last_data_time: 0.0068   lr: 0.01  max_mem: 4776M
[02/28 15:50:59 d2.utils.events]:  eta: 0:06:13  iter: 2479  total_loss: 0.4498  loss_cls: 0.04219  loss_box_reg: 0.1827  loss_mask: 0.1055  loss_rpn_cls: 0.006085  loss_rpn_loc: 0.09382    time: 0.7103  last_time: 0.7099  data_time: 0.0111  last_data_time: 0.0171   lr: 0.01  max_mem: 4776M
[02/28 15:51:13 d2.utils.events]:  eta: 0:05:59  iter: 2499  total_loss: 0.4544  loss_cls: 0.02976  loss_box_reg: 0.1905  loss_mask: 0.1271  loss_rpn_cls: 0.008237  loss_rpn_loc: 0.1059    time: 0.7104  last_time: 0.6110  data_time: 0.0162  last_data_time: 0.0108   lr: 0.01  max_mem: 4776M
[02/28 15:51:27 d2.utils.events]:  eta: 0:05:44  iter: 2519  total_loss: 0.4636  loss_cls: 0.04368  loss_box_reg: 0.2006  loss_mask: 0.12  loss_rpn_cls: 0.006101  loss_rpn_loc: 0.1062    time: 0.7102  last_time: 0.6922  data_time: 0.0107  last_data_time: 0.0147   lr: 0.01  max_mem: 4776M
[02/28 15:51:42 d2.utils.events]:  eta: 0:05:30  iter: 2539  total_loss: 0.4533  loss_cls: 0.03114  loss_box_reg: 0.1863  loss_mask: 0.1224  loss_rpn_cls: 0.008398  loss_rpn_loc: 0.09745    time: 0.7105  last_time: 0.8874  data_time: 0.0126  last_data_time: 0.0197   lr: 0.01  max_mem: 4776M
[02/28 15:51:56 d2.utils.events]:  eta: 0:05:15  iter: 2559  total_loss: 0.3978  loss_cls: 0.03404  loss_box_reg: 0.164  loss_mask: 0.1136  loss_rpn_cls: 0.004999  loss_rpn_loc: 0.08953    time: 0.7105  last_time: 0.7141  data_time: 0.0203  last_data_time: 0.0067   lr: 0.01  max_mem: 4776M
[02/28 15:52:10 d2.utils.events]:  eta: 0:05:02  iter: 2579  total_loss: 0.4108  loss_cls: 0.03885  loss_box_reg: 0.1711  loss_mask: 0.1142  loss_rpn_cls: 0.006083  loss_rpn_loc: 0.08575    time: 0.7105  last_time: 0.6868  data_time: 0.0153  last_data_time: 0.0139   lr: 0.01  max_mem: 4776M
[02/28 15:52:24 d2.utils.events]:  eta: 0:04:46  iter: 2599  total_loss: 0.417  loss_cls: 0.03496  loss_box_reg: 0.17  loss_mask: 0.1175  loss_rpn_cls: 0.005742  loss_rpn_loc: 0.0904    time: 0.7105  last_time: 0.7619  data_time: 0.0132  last_data_time: 0.0264   lr: 0.01  max_mem: 4776M
[02/28 15:52:39 d2.utils.events]:  eta: 0:04:32  iter: 2619  total_loss: 0.4236  loss_cls: 0.04085  loss_box_reg: 0.1666  loss_mask: 0.1132  loss_rpn_cls: 0.005748  loss_rpn_loc: 0.09233    time: 0.7105  last_time: 0.7767  data_time: 0.0144  last_data_time: 0.0120   lr: 0.01  max_mem: 4776M
[02/28 15:52:53 d2.utils.events]:  eta: 0:04:18  iter: 2639  total_loss: 0.4191  loss_cls: 0.03408  loss_box_reg: 0.1694  loss_mask: 0.1172  loss_rpn_cls: 0.005931  loss_rpn_loc: 0.09007    time: 0.7104  last_time: 0.5762  data_time: 0.0135  last_data_time: 0.0128   lr: 0.01  max_mem: 4776M
[02/28 15:53:07 d2.utils.events]:  eta: 0:04:03  iter: 2659  total_loss: 0.4043  loss_cls: 0.03751  loss_box_reg: 0.1657  loss_mask: 0.1135  loss_rpn_cls: 0.006044  loss_rpn_loc: 0.08908    time: 0.7103  last_time: 0.8076  data_time: 0.0125  last_data_time: 0.0056   lr: 0.01  max_mem: 4776M
[02/28 15:53:21 d2.utils.events]:  eta: 0:03:49  iter: 2679  total_loss: 0.4059  loss_cls: 0.03417  loss_box_reg: 0.1678  loss_mask: 0.1153  loss_rpn_cls: 0.005114  loss_rpn_loc: 0.09108    time: 0.7104  last_time: 0.7297  data_time: 0.0144  last_data_time: 0.0053   lr: 0.01  max_mem: 4776M
[02/28 15:53:36 d2.utils.events]:  eta: 0:03:35  iter: 2699  total_loss: 0.4089  loss_cls: 0.02654  loss_box_reg: 0.1706  loss_mask: 0.1216  loss_rpn_cls: 0.004598  loss_rpn_loc: 0.09    time: 0.7105  last_time: 0.7004  data_time: 0.0159  last_data_time: 0.0055   lr: 0.01  max_mem: 4776M
[02/28 15:53:50 d2.utils.events]:  eta: 0:03:21  iter: 2719  total_loss: 0.3943  loss_cls: 0.0295  loss_box_reg: 0.1607  loss_mask: 0.1091  loss_rpn_cls: 0.004306  loss_rpn_loc: 0.09098    time: 0.7105  last_time: 0.7268  data_time: 0.0134  last_data_time: 0.0160   lr: 0.01  max_mem: 4776M
[02/28 15:54:04 d2.utils.events]:  eta: 0:03:06  iter: 2739  total_loss: 0.4323  loss_cls: 0.04409  loss_box_reg: 0.1808  loss_mask: 0.1083  loss_rpn_cls: 0.005634  loss_rpn_loc: 0.09101    time: 0.7105  last_time: 0.6104  data_time: 0.0123  last_data_time: 0.0079   lr: 0.01  max_mem: 4776M
[02/28 15:54:19 d2.utils.events]:  eta: 0:02:52  iter: 2759  total_loss: 0.4233  loss_cls: 0.02459  loss_box_reg: 0.1773  loss_mask: 0.1216  loss_rpn_cls: 0.007989  loss_rpn_loc: 0.09688    time: 0.7106  last_time: 0.7275  data_time: 0.0126  last_data_time: 0.0087   lr: 0.01  max_mem: 4776M
[02/28 15:54:33 d2.utils.events]:  eta: 0:02:38  iter: 2779  total_loss: 0.4248  loss_cls: 0.03815  loss_box_reg: 0.1711  loss_mask: 0.1059  loss_rpn_cls: 0.005251  loss_rpn_loc: 0.09639    time: 0.7106  last_time: 0.5077  data_time: 0.0123  last_data_time: 0.0079   lr: 0.01  max_mem: 4776M
[02/28 15:54:47 d2.utils.events]:  eta: 0:02:23  iter: 2799  total_loss: 0.4211  loss_cls: 0.03787  loss_box_reg: 0.17  loss_mask: 0.111  loss_rpn_cls: 0.006022  loss_rpn_loc: 0.08892    time: 0.7105  last_time: 0.6383  data_time: 0.0121  last_data_time: 0.0150   lr: 0.01  max_mem: 4776M
[02/28 15:55:01 d2.utils.events]:  eta: 0:02:09  iter: 2819  total_loss: 0.4329  loss_cls: 0.03132  loss_box_reg: 0.1783  loss_mask: 0.1167  loss_rpn_cls: 0.007174  loss_rpn_loc: 0.09431    time: 0.7106  last_time: 0.7012  data_time: 0.0125  last_data_time: 0.0067   lr: 0.01  max_mem: 4776M
[02/28 15:55:16 d2.utils.events]:  eta: 0:01:54  iter: 2839  total_loss: 0.393  loss_cls: 0.03774  loss_box_reg: 0.1593  loss_mask: 0.1111  loss_rpn_cls: 0.005224  loss_rpn_loc: 0.0886    time: 0.7106  last_time: 0.7786  data_time: 0.0143  last_data_time: 0.0283   lr: 0.01  max_mem: 4776M
[02/28 15:55:29 d2.utils.events]:  eta: 0:01:40  iter: 2859  total_loss: 0.3994  loss_cls: 0.02912  loss_box_reg: 0.165  loss_mask: 0.1097  loss_rpn_cls: 0.005327  loss_rpn_loc: 0.08568    time: 0.7105  last_time: 0.7409  data_time: 0.0122  last_data_time: 0.0194   lr: 0.01  max_mem: 4776M
[02/28 15:55:43 d2.utils.events]:  eta: 0:01:26  iter: 2879  total_loss: 0.4031  loss_cls: 0.03027  loss_box_reg: 0.1659  loss_mask: 0.1101  loss_rpn_cls: 0.005115  loss_rpn_loc: 0.08834    time: 0.7104  last_time: 0.6151  data_time: 0.0163  last_data_time: 0.0132   lr: 0.01  max_mem: 4776M
[02/28 15:55:58 d2.utils.events]:  eta: 0:01:11  iter: 2899  total_loss: 0.4127  loss_cls: 0.03417  loss_box_reg: 0.1731  loss_mask: 0.1132  loss_rpn_cls: 0.004907  loss_rpn_loc: 0.09892    time: 0.7105  last_time: 0.7522  data_time: 0.0138  last_data_time: 0.0031   lr: 0.01  max_mem: 4776M
[02/28 15:56:12 d2.utils.events]:  eta: 0:00:57  iter: 2919  total_loss: 0.3834  loss_cls: 0.04225  loss_box_reg: 0.1549  loss_mask: 0.1076  loss_rpn_cls: 0.005005  loss_rpn_loc: 0.07899    time: 0.7104  last_time: 0.7281  data_time: 0.0145  last_data_time: 0.0159   lr: 0.01  max_mem: 4776M
[02/28 15:56:26 d2.utils.events]:  eta: 0:00:43  iter: 2939  total_loss: 0.4103  loss_cls: 0.03533  loss_box_reg: 0.1753  loss_mask: 0.1146  loss_rpn_cls: 0.006973  loss_rpn_loc: 0.09236    time: 0.7104  last_time: 0.7081  data_time: 0.0141  last_data_time: 0.0080   lr: 0.01  max_mem: 4776M
[02/28 15:56:40 d2.utils.events]:  eta: 0:00:28  iter: 2959  total_loss: 0.4137  loss_cls: 0.03966  loss_box_reg: 0.165  loss_mask: 0.1173  loss_rpn_cls: 0.004306  loss_rpn_loc: 0.08512    time: 0.7104  last_time: 0.5699  data_time: 0.0121  last_data_time: 0.0064   lr: 0.01  max_mem: 4776M
[02/28 15:56:55 d2.utils.events]:  eta: 0:00:14  iter: 2979  total_loss: 0.3899  loss_cls: 0.03477  loss_box_reg: 0.1565  loss_mask: 0.1086  loss_rpn_cls: 0.004177  loss_rpn_loc: 0.08545    time: 0.7105  last_time: 0.6905  data_time: 0.0138  last_data_time: 0.0224   lr: 0.01  max_mem: 4776M
[02/28 15:57:10 d2.utils.events]:  eta: 0:00:00  iter: 2999  total_loss: 0.4028  loss_cls: 0.02429  loss_box_reg: 0.1677  loss_mask: 0.118  loss_rpn_cls: 0.004435  loss_rpn_loc: 0.09053    time: 0.7105  last_time: 0.6751  data_time: 0.0150  last_data_time: 0.0040   lr: 0.01  max_mem: 4776M
[02/28 15:57:11 d2.engine.hooks]: Overall training speed: 2998 iterations in 0:35:30 (0.7106 s / it)
[02/28 15:57:11 d2.engine.hooks]: Total training time: 0:35:37 (0:00:07 on hooks)

[02/28 16:13:46 d2.data.build]: Removed 0 images with no usable annotations. 56 images left.
[02/28 16:13:46 d2.data.dataset_mapper]: [DatasetMapper] Augmentations used in training: [ResizeShortestEdge(short_edge_length=(640, 672, 704, 736, 768, 800), max_size=1333, sample_style='choice'), RandomFlip()]
[02/28 16:13:46 d2.data.build]: Using training sampler TrainingSampler
[02/28 16:13:46 d2.data.common]: Serializing the dataset using: <class 'detectron2.data.common._TorchSerializedList'>
[02/28 16:13:46 d2.data.common]: Serializing 56 elements to byte tensors and concatenating them all ...
[02/28 16:13:46 d2.data.common]: Serialized dataset takes 2.38 MiB
[02/28 16:13:46 d2.data.build]: Making batched data loader with batch_size=2
[02/28 16:13:46 d2.checkpoint.detection_checkpoint]: [DetectionCheckpointer] Loading from ./output/model_final.pth ...

/usr/local/lib/python3.11/dist-packages/fvcore/common/checkpoint.py:252: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  return torch.load(f, map_location=torch.device("cpu"))

[02/28 16:13:46 d2.engine.hooks]: Loading scheduler from state_dict ...
[02/28 16:13:46 d2.engine.train_loop]: Starting training from iteration 3000
[02/28 16:14:01 d2.utils.events]:  eta: 0:11:01  iter: 3019  total_loss: 0.3892  loss_cls: 0.03201  loss_box_reg: 0.1542  loss_mask: 0.1143  loss_rpn_cls: 0.00411  loss_rpn_loc: 0.08708    time: 0.6794  last_time: 0.8265  data_time: 0.0613  last_data_time: 0.0239   lr: 0.01  max_mem: 4776M
[02/28 16:14:15 d2.utils.events]:  eta: 0:11:01  iter: 3039  total_loss: 0.4017  loss_cls: 0.03379  loss_box_reg: 0.1611  loss_mask: 0.1121  loss_rpn_cls: 0.004829  loss_rpn_loc: 0.09186    time: 0.6953  last_time: 0.7207  data_time: 0.0128  last_data_time: 0.0015   lr: 0.01  max_mem: 4776M
[02/28 16:14:30 d2.utils.events]:  eta: 0:10:54  iter: 3059  total_loss: 0.4137  loss_cls: 0.02928  loss_box_reg: 0.1696  loss_mask: 0.1142  loss_rpn_cls: 0.004717  loss_rpn_loc: 0.09461    time: 0.7040  last_time: 0.7885  data_time: 0.0142  last_data_time: 0.0068   lr: 0.01  max_mem: 4776M
[02/28 16:14:44 d2.utils.events]:  eta: 0:10:52  iter: 3079  total_loss: 0.404  loss_cls: 0.03713  loss_box_reg: 0.1573  loss_mask: 0.113  loss_rpn_cls: 0.00529  loss_rpn_loc: 0.09234    time: 0.7078  last_time: 0.7129  data_time: 0.0122  last_data_time: 0.0078   lr: 0.01  max_mem: 4776M
[02/28 16:14:58 d2.utils.events]:  eta: 0:10:40  iter: 3099  total_loss: 0.3998  loss_cls: 0.02249  loss_box_reg: 0.1611  loss_mask: 0.1131  loss_rpn_cls: 0.004454  loss_rpn_loc: 0.09496    time: 0.7104  last_time: 0.7707  data_time: 0.0110  last_data_time: 0.0058   lr: 0.01  max_mem: 4776M
[02/28 16:15:13 d2.utils.events]:  eta: 0:10:28  iter: 3119  total_loss: 0.3913  loss_cls: 0.03613  loss_box_reg: 0.1559  loss_mask: 0.1149  loss_rpn_cls: 0.004381  loss_rpn_loc: 0.08996    time: 0.7154  last_time: 0.7929  data_time: 0.0170  last_data_time: 0.0181   lr: 0.01  max_mem: 4776M
[02/28 16:15:27 d2.utils.events]:  eta: 0:10:15  iter: 3139  total_loss: 0.3781  loss_cls: 0.03913  loss_box_reg: 0.1507  loss_mask: 0.09506  loss_rpn_cls: 0.00361  loss_rpn_loc: 0.08079    time: 0.7151  last_time: 0.7356  data_time: 0.0116  last_data_time: 0.0124   lr: 0.01  max_mem: 4776M
[02/28 16:15:42 d2.utils.events]:  eta: 0:10:02  iter: 3159  total_loss: 0.3932  loss_cls: 0.03059  loss_box_reg: 0.1573  loss_mask: 0.1141  loss_rpn_cls: 0.004771  loss_rpn_loc: 0.09105    time: 0.7170  last_time: 0.9145  data_time: 0.0178  last_data_time: 0.1106   lr: 0.01  max_mem: 4776M
[02/28 16:15:57 d2.utils.events]:  eta: 0:09:49  iter: 3179  total_loss: 0.3962  loss_cls: 0.02722  loss_box_reg: 0.1632  loss_mask: 0.1158  loss_rpn_cls: 0.004385  loss_rpn_loc: 0.08234    time: 0.7174  last_time: 0.7911  data_time: 0.0144  last_data_time: 0.0116   lr: 0.01  max_mem: 4776M
[02/28 16:16:11 d2.utils.events]:  eta: 0:09:34  iter: 3199  total_loss: 0.3688  loss_cls: 0.03481  loss_box_reg: 0.1527  loss_mask: 0.09731  loss_rpn_cls: 0.003253  loss_rpn_loc: 0.07907    time: 0.7158  last_time: 0.8019  data_time: 0.0136  last_data_time: 0.0187   lr: 0.01  max_mem: 4776M
[02/28 16:16:25 d2.utils.events]:  eta: 0:09:21  iter: 3219  total_loss: 0.3774  loss_cls: 0.02718  loss_box_reg: 0.1559  loss_mask: 0.1068  loss_rpn_cls: 0.004236  loss_rpn_loc: 0.08535    time: 0.7169  last_time: 0.5572  data_time: 0.0147  last_data_time: 0.0018   lr: 0.01  max_mem: 4776M
[02/28 16:16:40 d2.utils.events]:  eta: 0:09:08  iter: 3239  total_loss: 0.3739  loss_cls: 0.02753  loss_box_reg: 0.1505  loss_mask: 0.1027  loss_rpn_cls: 0.003601  loss_rpn_loc: 0.08336    time: 0.7188  last_time: 0.5651  data_time: 0.0146  last_data_time: 0.0050   lr: 0.01  max_mem: 4776M
[02/28 16:16:54 d2.utils.events]:  eta: 0:08:54  iter: 3259  total_loss: 0.3901  loss_cls: 0.02682  loss_box_reg: 0.1511  loss_mask: 0.1007  loss_rpn_cls: 0.005699  loss_rpn_loc: 0.08959    time: 0.7180  last_time: 0.7478  data_time: 0.0142  last_data_time: 0.0245   lr: 0.01  max_mem: 4776M
[02/28 16:17:08 d2.utils.events]:  eta: 0:08:41  iter: 3279  total_loss: 0.3954  loss_cls: 0.02292  loss_box_reg: 0.1607  loss_mask: 0.1166  loss_rpn_cls: 0.005236  loss_rpn_loc: 0.09238    time: 0.7167  last_time: 0.5204  data_time: 0.0133  last_data_time: 0.0039   lr: 0.01  max_mem: 4776M
[02/28 16:17:22 d2.utils.events]:  eta: 0:08:24  iter: 3299  total_loss: 0.3808  loss_cls: 0.03092  loss_box_reg: 0.1573  loss_mask: 0.1024  loss_rpn_cls: 0.00494  loss_rpn_loc: 0.08348    time: 0.7160  last_time: 0.6493  data_time: 0.0123  last_data_time: 0.0072   lr: 0.01  max_mem: 4776M
[02/28 16:17:36 d2.utils.events]:  eta: 0:08:10  iter: 3319  total_loss: 0.3805  loss_cls: 0.02439  loss_box_reg: 0.1561  loss_mask: 0.1075  loss_rpn_cls: 0.003888  loss_rpn_loc: 0.08561    time: 0.7155  last_time: 0.6537  data_time: 0.0129  last_data_time: 0.0059   lr: 0.01  max_mem: 4776M
[02/28 16:17:51 d2.utils.events]:  eta: 0:07:56  iter: 3339  total_loss: 0.3938  loss_cls: 0.02532  loss_box_reg: 0.1651  loss_mask: 0.1147  loss_rpn_cls: 0.005218  loss_rpn_loc: 0.08526    time: 0.7160  last_time: 0.8186  data_time: 0.0142  last_data_time: 0.0160   lr: 0.01  max_mem: 4776M
[02/28 16:18:06 d2.utils.events]:  eta: 0:07:43  iter: 3359  total_loss: 0.3746  loss_cls: 0.02441  loss_box_reg: 0.1541  loss_mask: 0.1093  loss_rpn_cls: 0.004568  loss_rpn_loc: 0.08416    time: 0.7167  last_time: 0.7432  data_time: 0.0138  last_data_time: 0.0062   lr: 0.01  max_mem: 4776M
[02/28 16:18:20 d2.utils.events]:  eta: 0:07:28  iter: 3379  total_loss: 0.3678  loss_cls: 0.03103  loss_box_reg: 0.1419  loss_mask: 0.1028  loss_rpn_cls: 0.004569  loss_rpn_loc: 0.07532    time: 0.7169  last_time: 0.7312  data_time: 0.0140  last_data_time: 0.0182   lr: 0.01  max_mem: 4776M
[02/28 16:18:35 d2.utils.events]:  eta: 0:07:14  iter: 3399  total_loss: 0.4004  loss_cls: 0.0305  loss_box_reg: 0.1646  loss_mask: 0.1094  loss_rpn_cls: 0.004322  loss_rpn_loc: 0.08554    time: 0.7177  last_time: 0.7538  data_time: 0.0139  last_data_time: 0.0436   lr: 0.01  max_mem: 4776M
[02/28 16:18:48 d2.utils.events]:  eta: 0:06:59  iter: 3419  total_loss: 0.3774  loss_cls: 0.02594  loss_box_reg: 0.1537  loss_mask: 0.1123  loss_rpn_cls: 0.004475  loss_rpn_loc: 0.08619    time: 0.7161  last_time: 0.6649  data_time: 0.0116  last_data_time: 0.0025   lr: 0.01  max_mem: 4776M
[02/28 16:19:02 d2.utils.events]:  eta: 0:06:43  iter: 3439  total_loss: 0.4016  loss_cls: 0.03276  loss_box_reg: 0.1624  loss_mask: 0.1068  loss_rpn_cls: 0.006823  loss_rpn_loc: 0.08805    time: 0.7151  last_time: 0.6985  data_time: 0.0155  last_data_time: 0.0187   lr: 0.01  max_mem: 4776M
[02/28 16:19:16 d2.utils.events]:  eta: 0:06:29  iter: 3459  total_loss: 0.36  loss_cls: 0.03354  loss_box_reg: 0.157  loss_mask: 0.09425  loss_rpn_cls: 0.003553  loss_rpn_loc: 0.07538    time: 0.7147  last_time: 0.5691  data_time: 0.0135  last_data_time: 0.0039   lr: 0.01  max_mem: 4776M
[02/28 16:19:31 d2.utils.events]:  eta: 0:06:15  iter: 3479  total_loss: 0.3679  loss_cls: 0.03012  loss_box_reg: 0.1566  loss_mask: 0.1017  loss_rpn_cls: 0.004572  loss_rpn_loc: 0.08589    time: 0.7148  last_time: 0.7214  data_time: 0.0145  last_data_time: 0.0252   lr: 0.01  max_mem: 4776M
[02/28 16:19:45 d2.utils.events]:  eta: 0:06:01  iter: 3499  total_loss: 0.3777  loss_cls: 0.03056  loss_box_reg: 0.1476  loss_mask: 0.103  loss_rpn_cls: 0.003649  loss_rpn_loc: 0.08183    time: 0.7157  last_time: 0.6997  data_time: 0.0157  last_data_time: 0.0240   lr: 0.01  max_mem: 4776M
[02/28 16:20:00 d2.utils.events]:  eta: 0:05:47  iter: 3519  total_loss: 0.4055  loss_cls: 0.03573  loss_box_reg: 0.1556  loss_mask: 0.1036  loss_rpn_cls: 0.006052  loss_rpn_loc: 0.0989    time: 0.7162  last_time: 0.5519  data_time: 0.0155  last_data_time: 0.0060   lr: 0.01  max_mem: 4776M
[02/28 16:20:15 d2.utils.events]:  eta: 0:05:33  iter: 3539  total_loss: 0.3832  loss_cls: 0.03026  loss_box_reg: 0.1504  loss_mask: 0.1115  loss_rpn_cls: 0.004935  loss_rpn_loc: 0.09258    time: 0.7167  last_time: 0.6885  data_time: 0.0151  last_data_time: 0.0020   lr: 0.01  max_mem: 4776M
[02/28 16:20:29 d2.utils.events]:  eta: 0:05:18  iter: 3559  total_loss: 0.3545  loss_cls: 0.03117  loss_box_reg: 0.1448  loss_mask: 0.09909  loss_rpn_cls: 0.005542  loss_rpn_loc: 0.07676    time: 0.7168  last_time: 0.6444  data_time: 0.0133  last_data_time: 0.0058   lr: 0.01  max_mem: 4776M
[02/28 16:20:44 d2.utils.events]:  eta: 0:05:04  iter: 3579  total_loss: 0.3814  loss_cls: 0.02963  loss_box_reg: 0.1545  loss_mask: 0.1084  loss_rpn_cls: 0.004582  loss_rpn_loc: 0.08037    time: 0.7171  last_time: 0.7241  data_time: 0.0118  last_data_time: 0.0067   lr: 0.01  max_mem: 4776M
[02/28 16:20:58 d2.utils.events]:  eta: 0:04:49  iter: 3599  total_loss: 0.3825  loss_cls: 0.02804  loss_box_reg: 0.158  loss_mask: 0.1196  loss_rpn_cls: 0.004237  loss_rpn_loc: 0.08834    time: 0.7176  last_time: 0.7819  data_time: 0.0160  last_data_time: 0.0179   lr: 0.01  max_mem: 4776M
[02/28 16:21:12 d2.utils.events]:  eta: 0:04:35  iter: 3619  total_loss: 0.3579  loss_cls: 0.02362  loss_box_reg: 0.1474  loss_mask: 0.1036  loss_rpn_cls: 0.003513  loss_rpn_loc: 0.07897    time: 0.7173  last_time: 0.6357  data_time: 0.0136  last_data_time: 0.0217   lr: 0.01  max_mem: 4776M
[02/28 16:21:26 d2.utils.events]:  eta: 0:04:20  iter: 3639  total_loss: 0.3607  loss_cls: 0.03188  loss_box_reg: 0.1438  loss_mask: 0.1061  loss_rpn_cls: 0.004457  loss_rpn_loc: 0.08171    time: 0.7169  last_time: 0.6330  data_time: 0.0144  last_data_time: 0.0149   lr: 0.01  max_mem: 4776M
[02/28 16:21:41 d2.utils.events]:  eta: 0:04:06  iter: 3659  total_loss: 0.375  loss_cls: 0.03282  loss_box_reg: 0.1579  loss_mask: 0.1101  loss_rpn_cls: 0.003607  loss_rpn_loc: 0.08269    time: 0.7174  last_time: 0.6408  data_time: 0.0157  last_data_time: 0.0070   lr: 0.01  max_mem: 4776M
[02/28 16:21:55 d2.utils.events]:  eta: 0:03:51  iter: 3679  total_loss: 0.3507  loss_cls: 0.0296  loss_box_reg: 0.1481  loss_mask: 0.1014  loss_rpn_cls: 0.00328  loss_rpn_loc: 0.07684    time: 0.7167  last_time: 0.5686  data_time: 0.0120  last_data_time: 0.0045   lr: 0.01  max_mem: 4776M
[02/28 16:22:10 d2.utils.events]:  eta: 0:03:37  iter: 3699  total_loss: 0.3662  loss_cls: 0.02577  loss_box_reg: 0.146  loss_mask: 0.1088  loss_rpn_cls: 0.003689  loss_rpn_loc: 0.08272    time: 0.7175  last_time: 0.7729  data_time: 0.0140  last_data_time: 0.0052   lr: 0.01  max_mem: 4776M
[02/28 16:22:24 d2.utils.events]:  eta: 0:03:22  iter: 3719  total_loss: 0.3939  loss_cls: 0.02816  loss_box_reg: 0.1603  loss_mask: 0.1094  loss_rpn_cls: 0.005467  loss_rpn_loc: 0.08921    time: 0.7176  last_time: 0.7483  data_time: 0.0157  last_data_time: 0.0164   lr: 0.01  max_mem: 4776M
[02/28 16:22:38 d2.utils.events]:  eta: 0:03:08  iter: 3739  total_loss: 0.3857  loss_cls: 0.03852  loss_box_reg: 0.1516  loss_mask: 0.1063  loss_rpn_cls: 0.005134  loss_rpn_loc: 0.08439    time: 0.7169  last_time: 0.8095  data_time: 0.0128  last_data_time: 0.0072   lr: 0.01  max_mem: 4776M
[02/28 16:22:53 d2.utils.events]:  eta: 0:02:53  iter: 3759  total_loss: 0.3535  loss_cls: 0.0364  loss_box_reg: 0.142  loss_mask: 0.09833  loss_rpn_cls: 0.003571  loss_rpn_loc: 0.07607    time: 0.7169  last_time: 0.8003  data_time: 0.0123  last_data_time: 0.0235   lr: 0.01  max_mem: 4776M
[02/28 16:23:07 d2.utils.events]:  eta: 0:02:39  iter: 3779  total_loss: 0.3926  loss_cls: 0.02249  loss_box_reg: 0.1543  loss_mask: 0.1089  loss_rpn_cls: 0.004392  loss_rpn_loc: 0.0884    time: 0.7169  last_time: 0.6818  data_time: 0.0158  last_data_time: 0.0244   lr: 0.01  max_mem: 4776M
[02/28 16:23:21 d2.utils.events]:  eta: 0:02:24  iter: 3799  total_loss: 0.3784  loss_cls: 0.02677  loss_box_reg: 0.154  loss_mask: 0.1006  loss_rpn_cls: 0.003182  loss_rpn_loc: 0.07466    time: 0.7170  last_time: 0.5993  data_time: 0.0161  last_data_time: 0.0091   lr: 0.01  max_mem: 4776M
[02/28 16:23:36 d2.utils.events]:  eta: 0:02:10  iter: 3819  total_loss: 0.3564  loss_cls: 0.02875  loss_box_reg: 0.1383  loss_mask: 0.09479  loss_rpn_cls: 0.002676  loss_rpn_loc: 0.08546    time: 0.7167  last_time: 0.5660  data_time: 0.0119  last_data_time: 0.0143   lr: 0.01  max_mem: 4776M
[02/28 16:23:50 d2.utils.events]:  eta: 0:01:55  iter: 3839  total_loss: 0.3732  loss_cls: 0.03217  loss_box_reg: 0.1523  loss_mask: 0.1096  loss_rpn_cls: 0.004643  loss_rpn_loc: 0.08184    time: 0.7168  last_time: 0.7119  data_time: 0.0145  last_data_time: 0.0116   lr: 0.01  max_mem: 4776M
[02/28 16:24:04 d2.utils.events]:  eta: 0:01:41  iter: 3859  total_loss: 0.374  loss_cls: 0.03136  loss_box_reg: 0.1535  loss_mask: 0.1058  loss_rpn_cls: 0.005625  loss_rpn_loc: 0.08081    time: 0.7168  last_time: 0.7665  data_time: 0.0124  last_data_time: 0.0042   lr: 0.01  max_mem: 4776M
[02/28 16:24:19 d2.utils.events]:  eta: 0:01:26  iter: 3879  total_loss: 0.3519  loss_cls: 0.02482  loss_box_reg: 0.142  loss_mask: 0.09954  loss_rpn_cls: 0.003927  loss_rpn_loc: 0.07962    time: 0.7172  last_time: 0.7252  data_time: 0.0139  last_data_time: 0.0169   lr: 0.01  max_mem: 4776M
[02/28 16:24:34 d2.utils.events]:  eta: 0:01:12  iter: 3899  total_loss: 0.3479  loss_cls: 0.03111  loss_box_reg: 0.1371  loss_mask: 0.09616  loss_rpn_cls: 0.002893  loss_rpn_loc: 0.07834    time: 0.7175  last_time: 0.7444  data_time: 0.0141  last_data_time: 0.0189   lr: 0.01  max_mem: 4776M
[02/28 16:24:48 d2.utils.events]:  eta: 0:00:57  iter: 3919  total_loss: 0.3492  loss_cls: 0.02823  loss_box_reg: 0.1375  loss_mask: 0.1006  loss_rpn_cls: 0.003831  loss_rpn_loc: 0.08476    time: 0.7174  last_time: 0.7509  data_time: 0.0126  last_data_time: 0.0070   lr: 0.01  max_mem: 4776M
[02/28 16:25:03 d2.utils.events]:  eta: 0:00:43  iter: 3939  total_loss: 0.361  loss_cls: 0.02061  loss_box_reg: 0.1456  loss_mask: 0.1083  loss_rpn_cls: 0.004208  loss_rpn_loc: 0.07837    time: 0.7178  last_time: 0.6480  data_time: 0.0177  last_data_time: 0.0105   lr: 0.01  max_mem: 4776M
[02/28 16:25:17 d2.utils.events]:  eta: 0:00:28  iter: 3959  total_loss: 0.3295  loss_cls: 0.02732  loss_box_reg: 0.1339  loss_mask: 0.08826  loss_rpn_cls: 0.002616  loss_rpn_loc: 0.07424    time: 0.7175  last_time: 0.6092  data_time: 0.0128  last_data_time: 0.0071   lr: 0.01  max_mem: 4776M
[02/28 16:25:31 d2.utils.events]:  eta: 0:00:14  iter: 3979  total_loss: 0.3761  loss_cls: 0.03009  loss_box_reg: 0.1541  loss_mask: 0.1054  loss_rpn_cls: 0.003981  loss_rpn_loc: 0.08014    time: 0.7176  last_time: 0.7103  data_time: 0.0127  last_data_time: 0.0088   lr: 0.01  max_mem: 4776M
[02/28 16:25:46 d2.utils.events]:  eta: 0:00:00  iter: 3999  total_loss: 0.3582  loss_cls: 0.03499  loss_box_reg: 0.1449  loss_mask: 0.1035  loss_rpn_cls: 0.003509  loss_rpn_loc: 0.07583    time: 0.7171  last_time: 0.6445  data_time: 0.0129  last_data_time: 0.0035   lr: 0.01  max_mem: 4776M
[02/28 16:25:47 d2.engine.hooks]: Overall training speed: 998 iterations in 0:11:55 (0.7171 s / it)
[02/28 16:25:47 d2.engine.hooks]: Total training time: 0:11:57 (0:00:02 on hooks)



